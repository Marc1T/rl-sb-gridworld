# ğŸ§  GridWorld Reinforcement Learning Project

## ğŸ¯ Objectif du projet

DÃ©velopper et entraÃ®ner des agents dâ€™apprentissage par renforcement (RL) capables dâ€™Ã©voluer dans un environnement **GridWorld** configurable, de la version simple (v1) Ã  la version multi-agents compÃ©titive (v3).
Le projet dÃ©montre lâ€™usage de **Stable-Baselines3**, de **Gymnasium**, et de **RL-Baselines3-Zoo** dans un cadre modulaire et extensible.

---

## ğŸ—‚ï¸ Structure du projet

```bash
gridworld_project/
â”‚
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py                  # Enregistrement des environnements Gym
â”‚   â”œâ”€â”€ gridworld_env_v1.py          # Environnement simple (1 agent, 1 but)
â”‚   â”œâ”€â”€ gridworld_env_v2.py          # Version amÃ©liorÃ©e (configurable, obstacles)
â”‚   â”œâ”€â”€ gridworld_env_v3_multi.py    # Version multi-agents, compÃ©titive ou coopÃ©rative
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_ppo_v1.py              # EntraÃ®nement PPO pour GridWorld-v1
â”‚   â”œâ”€â”€ train_ppo_v2.py              # EntraÃ®nement PPO pour GridWorld-v2
â”‚   â”œâ”€â”€ train_ppo_v3.py              # EntraÃ®nement PPO multi-agents (v3)
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate_model_v1.py         # Ã‰valuation dâ€™un modÃ¨le entraÃ®nÃ© (v1)
â”‚   â”œâ”€â”€ evaluate_model_v2.py         # Ã‰valuation et rendu matplotlib (v2)
â”‚   â”œâ”€â”€ record_videos_v2.py          # GÃ©nÃ©ration de vidÃ©os MP4 via matplotlib
â”‚
â”œâ”€â”€ rl_zoo_configs/
â”‚   â”œâ”€â”€ GridWorld-v3.yml             # HyperparamÃ¨tres pour RL-Baselines3-Zoo
â”‚
â”œâ”€â”€ logs/                            # Journaux, snapshots de config, checkpoints
â”œâ”€â”€ tensorboard/                     # Logs TensorBoard
â”œâ”€â”€ train_gridworld_v3.py            # Wrapper pour utiliser RL Zoo directement
â””â”€â”€ README.md                        # (ce fichier)
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ CrÃ©er lâ€™environnement virtuel

```bash
conda create -n rl-sb python=3.12
conda activate rl-sb
```

### 2ï¸âƒ£ Installer les dÃ©pendances

Installe les packages nÃ©cessaires pour RL et lâ€™environnement GridWorld :

```bash
pip install stable-baselines3[extra] gymnasium matplotlib numpy
pip install rl-baselines3-zoo pygame
```

ğŸ’¡ Pour plus de dÃ©tails et options dâ€™installation, consulte la documentation officielle :

* **Stable-Baselines3** : [Quickstart Guide](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)
* **RL Baselines3 Zoo** : [Documentation](https://rl-baselines3-zoo.readthedocs.io/en/master/index.html)


### 3ï¸âƒ£ VÃ©rifier que tout est fonctionnel

```bash
python -c "import gymnasium as gym; import envs; env=gym.make('GridWorld-v3'); print(env.reset())"
```

---

## ğŸš€ EntraÃ®nement

### ğŸ”¹ Version 1 â€“ Basique

```bash
python training/train_ppo_v1.py
```

### ğŸ”¹ Version 2 â€“ Configurable avec obstacles

```bash
python training/train_ppo_v2.py
```

### ğŸ”¹ Version 3 â€“ Multi-agents (compÃ©titif/co-opÃ©ratif)

```bash
python training/train_ppo_v3.py
```

### ğŸ”¹ Avec RL-Baselines3-Zoo

```bash
python train_gridworld_v3.py --algo ppo --env GridWorld-v3 --conf rl_zoo_configs/GridWorld-v3.yml --tensorboard-log ./tensorboard/
```

---

## ğŸ§© Configuration RL Zoo

ğŸ“„ `rl_zoo_configs/GridWorld-v3.yml`

```yaml
GridWorld-v3:
  n_envs: 1
  n_timesteps: 100000
  policy: 'MlpPolicy'
  n_steps: 128
  batch_size: 64
  gamma: 0.99
  gae_lambda: 0.95
  ent_coef: 0.01
  learning_rate: 2.5e-4
  clip_range: 0.2
  n_epochs: 10
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: False
  normalize_advantage: True
  policy_kwargs: "dict(net_arch=[128, 128])"
```

## ğŸ“Š Visualiser les mÃ©triques avec TensorBoard

Chaque script dâ€™entraÃ®nement (`train_ppo_v1.py`, `train_ppo_v2.py`, `train_ppo_v3.py`) est configurÃ© pour logger les mÃ©triques vers TensorBoard. Tu peux ainsi suivre en temps rÃ©el :

* Reward moyen par Ã©pisode
* Policy loss et value loss
* Entropy
* Gradients et norm
* Et plus selon la version

### 1ï¸âƒ£ Lancer TensorBoard

Ouvre un terminal Ã  la racine du projet et exÃ©cute :

```bash
tensorboard --logdir ./tensorboard/
```

Par dÃ©faut, TensorBoard sâ€™ouvre sur [http://localhost:6006](http://localhost:6006).

---

### 2ï¸âƒ£ Organisation des logs

Pour ne pas mÃ©langer les versions, chaque script dâ€™entraÃ®nement Ã©crit dans un sous-dossier dÃ©diÃ© :

| Version | Script dâ€™entraÃ®nement           | Dossier TensorBoard     |
| ------- | ------------------------------- | ----------------------- |
| v1      | `train_ppo_v1.py`               | `./tensorboard/PPO_v1/` |
| v2      | `train_ppo_v2.py`               | `./tensorboard/PPO_v2/` |
| v3      | `train_ppo_v3.py` (multi-agent) | `./tensorboard/PPO_v3/` |

ğŸ’¡ Astuce : tu peux modifier la variable `tensorboard_log` dans le script pour changer le dossier de sortie.

---

### 3ï¸âƒ£ Exemple dâ€™utilisation

Pour entraÃ®ner et suivre v2 :

```bash
python training/train_ppo_v2.py
tensorboard --logdir ./tensorboard/PPO_v2/
```

Ouvre ensuite ton navigateur sur [http://localhost:6006](http://localhost:6006) pour visualiser les mÃ©triques en temps rÃ©el pendant lâ€™entraÃ®nement.

---

### 4ï¸âƒ£ Conseils pratiques

* Si tu relances un entraÃ®nement, TensorBoard fusionnera les nouveaux logs avec les anciens dans le mÃªme dossier.
* Pour comparer plusieurs versions, ouvre TensorBoard et coche plusieurs courbes (`PPO_v1`, `PPO_v2`, `PPO_v3`) simultanÃ©ment.

### ğŸ”¹ Enregistrement de vidÃ©os

ExÃ©cuter :

```bash
python evaluation/record_videos_v2.py
```

Les vidÃ©os seront sauvegardÃ©es dans `./videos/`.

---

## ğŸ§ª Ã‰valuation dâ€™un modÃ¨le

Exemple (version 2 ou 3) :

```bash
python evaluation/evaluate_model_v2.py --model_path ./logs/best_model.zip
```

Cela affiche le comportement de lâ€™agent dans la grille et peut gÃ©nÃ©rer un score moyen sur plusieurs Ã©pisodes.

---

## âš™ï¸ Exemple de configuration personnalisÃ©e

Tu peux crÃ©er des environnements paramÃ©trables :

```python
from envs.gridworld_env_v3_multi import GridWorldMultiAgentEnv
env = GridWorldMultiAgentEnv({
    "grid_size": 8,
    "n_agents": 3,
    "n_goals": 2,
    "n_obstacles": 5,
    "max_steps": 150,
    "obstacle_mode": "fixed"
})
obs, info = env.reset()
env.render()
```

---

## ğŸ§  Points clÃ©s du projet

* **v1** : un agent simple avec apprentissage basique.
* **v2** : ajout dâ€™obstacles, dâ€™une grille configurable et rendu visuel.
* **v3** : environnement **multi-agents** avec collisions, coopÃ©ration et compÃ©tition.
* **Support complet RL-Zoo** pour un entraÃ®nement reproductible.
* **ModularitÃ© maximale** â†’ tous les scripts sont indÃ©pendants et extensibles.

---

## ğŸ’¾ Sauvegarde & Reprise

Les modÃ¨les sont sauvegardÃ©s automatiquement dans :

```
./logs/PPO_GridWorld-vX_<timestamp>/best_model.zip
```

Pour reprendre un entraÃ®nement :

```python
from stable_baselines3 import PPO
model = PPO.load("logs/best_model.zip")
```

---

## ğŸ§© Auteur

ğŸ‘¨â€ğŸ’» **Marc Thierry Nankouli**
Ã‰lÃ¨ve ingÃ©nieur en IA et Data Technologies
Projet personnel de recherche en apprentissage par renforcement et conception dâ€™environnements simulÃ©s.
